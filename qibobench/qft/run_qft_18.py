#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Qibo åç«¯åŸºå‡†æµ‹è¯•æ¡†æ¶ - å®Œæ•´æŒ‡æ ‡æŠ¥å‘Š
æ”¯æŒCSVã€Markdownã€JSONç­‰å¤šç§è¾“å‡ºæ ¼å¼
"""

import time
import sys
import os
import json
import csv
import platform
import psutil
import numpy as np
from datetime import datetime
from qibo import Circuit, gates, set_backend

# åŸºå‡†æµ‹è¯•æŒ‡æ ‡é…ç½®
class BenchmarkConfig:
    """åŸºå‡†æµ‹è¯•é…ç½®ç±»"""
    def __init__(self):
        self.num_runs = 5  # æ¯ä¸ªåç«¯è¿è¡Œæ¬¡æ•°ï¼ˆç”¨äºè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼‰
        self.warmup_runs = 1  # é¢„çƒ­è¿è¡Œæ¬¡æ•°
        self.output_formats = ['csv', 'markdown', 'json']  # è¾“å‡ºæ ¼å¼
        self.baseline_backend = "numpy"  # åŸºå‡†åç«¯ï¼ˆç”¨äºè®¡ç®—åŠ é€Ÿæ¯”ï¼‰

# åŸºå‡†æµ‹è¯•æŒ‡æ ‡ç±»
class BenchmarkMetrics:
    """å­˜å‚¨åŸºå‡†æµ‹è¯•æŒ‡æ ‡"""
    def __init__(self):
        # æ ¸å¿ƒæŒ‡æ ‡
        self.execution_time_mean = None
        self.execution_time_std = None
        self.peak_memory_mb = None
        self.speedup = None
        self.correctness = "Unknown"
        
        # é«˜ä¼˜å…ˆçº§æŒ‡æ ‡
        self.circuit_parameters = {}
        self.backend_info = {}
        
        # ä¸­ä¼˜å…ˆçº§æŒ‡æ ‡
        self.throughput_gates_per_sec = None
        self.jit_compilation_time = None
        self.environment_info = {}
        
        # ä½ä¼˜å…ˆçº§æŒ‡æ ‡
        self.circuit_build_time = None
        self.report_metadata = {}

# åŸºå‡†æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨
class BenchmarkReporter:
    """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
    
    @staticmethod
    def generate_csv_report(results, filename="qibobench/qft/benchmark_report.csv"):
        """ç”ŸæˆCSVæ ¼å¼æŠ¥å‘Š"""
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # å†™å…¥è¡¨å¤´
            headers = [
                "åç«¯åç§°", "æ‰§è¡Œæ—¶é—´å‡å€¼(ç§’)", "æ‰§è¡Œæ—¶é—´æ ‡å‡†å·®(ç§’)", 
                "å³°å€¼å†…å­˜å ç”¨(MB)", "åŠ é€Ÿæ¯”", "æ­£ç¡®æ€§éªŒè¯",
                "é‡å­æ¯”ç‰¹æ•°", "ç”µè·¯æ·±åº¦", "é—¨æ•°é‡", "ååç‡(é—¨/ç§’)",
                "JITç¼–è¯‘æ—¶é—´(ç§’)", "ç”µè·¯æ„å»ºæ—¶é—´(ç§’)"
            ]
            writer.writerow(headers)
            
            # å†™å…¥æ•°æ®
            for backend_name, metrics in results.items():
                if metrics.execution_time_mean is not None:
                    row = [
                        backend_name,
                        f"{metrics.execution_time_mean:.6f}",
                        f"{metrics.execution_time_std:.6f}",
                        f"{metrics.peak_memory_mb:.2f}",
                        f"{metrics.speedup:.2f}x" if metrics.speedup else "N/A",
                        metrics.correctness,
                        metrics.circuit_parameters.get('nqubits', 'N/A'),
                        metrics.circuit_parameters.get('depth', 'N/A'),
                        metrics.circuit_parameters.get('ngates', 'N/A'),
                        f"{metrics.throughput_gates_per_sec:.2f}" if metrics.throughput_gates_per_sec else "N/A",
                        f"{metrics.jit_compilation_time:.4f}" if metrics.jit_compilation_time else "N/A",
                        f"{metrics.circuit_build_time:.4f}" if metrics.circuit_build_time else "N/A"
                    ]
                    writer.writerow(row)
        
        print(f"CSVæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
    
    @staticmethod
    def generate_markdown_report(results, filename="qibobench/qft/benchmark_report.md"):
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as md_file:
            # æŠ¥å‘Šæ ‡é¢˜
            md_file.write("# Qibo åç«¯åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n\n")
            md_file.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æµ‹è¯•ç”µè·¯ä¿¡æ¯
            md_file.write("## æµ‹è¯•ç”µè·¯å‚æ•°\n\n")
            md_file.write("### QFT (Quantum Fourier Transform) ç”µè·¯\n\n")
            
            # ä»ç¬¬ä¸€ä¸ªæœ‰æ•ˆç»“æœä¸­è·å–ç”µè·¯å‚æ•°
            circuit_params = None
            for metrics in results.values():
                if hasattr(metrics, 'circuit_parameters') and metrics.circuit_parameters:
                    circuit_params = metrics.circuit_parameters
                    break
            
            if circuit_params:
                md_file.write("| å‚æ•° | å€¼ | æè¿° |\n")
                md_file.write("|------|----|------|\n")
                md_file.write(f"| ç”µè·¯ç±»å‹ | {circuit_params.get('type', 'QFT')} | é‡å­å‚…é‡Œå¶å˜æ¢ç”µè·¯ |\n")
                md_file.write(f"| é‡å­æ¯”ç‰¹æ•° | {circuit_params.get('nqubits', 'N/A')} | ç”µè·¯çš„å®½åº¦ |\n")
                md_file.write(f"| ç”µè·¯æ·±åº¦ | {circuit_params.get('depth', 'N/A')} | ç”µè·¯çš„å±‚æ•° |\n")
                md_file.write(f"| é—¨æ•°é‡ | {circuit_params.get('ngates', 'N/A')} | æ€»é—¨æ“ä½œæ•° |\n")
                md_file.write(f"| ç”µè·¯æ¥æº | {circuit_params.get('source', 'N/A')} | QASMæ–‡ä»¶è·¯å¾„ |\n")
                md_file.write("\n")
            
            # æµ‹è¯•é…ç½®
            md_file.write("### æµ‹è¯•é…ç½®\n\n")
            md_file.write("- **è¿è¡Œæ¬¡æ•°**: 5æ¬¡æ­£å¼è¿è¡Œ + 1æ¬¡é¢„çƒ­è¿è¡Œ\n")
            md_file.write("- **åŸºå‡†åç«¯**: numpy (ä½œä¸ºæ€§èƒ½æ¯”è¾ƒåŸºå‡†)\n")
            md_file.write("- **æµ‹è¯•ç›®æ ‡**: æ¯”è¾ƒä¸åŒåç«¯åœ¨ç›¸åŒQFTç”µè·¯ä¸Šçš„æ€§èƒ½è¡¨ç°\n")
            md_file.write("- **è¾“å‡ºæ ¼å¼**: CSV, Markdown, JSON\n\n")
            
            # æ ¸å¿ƒæŒ‡æ ‡è¡¨æ ¼
            md_file.write("## æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡\n\n")
            md_file.write("| ä¼˜å…ˆçº§ | æŒ‡æ ‡ | æè¿° | ç¤ºä¾‹ |\n")
            md_file.write("|--------|------|------|------|\n")
            md_file.write("| æ ¸å¿ƒ | æ‰§è¡Œæ—¶é—´ (å‡å€¼ Â± æ ‡å‡†å·®) | æœ€é‡è¦çš„æ€§èƒ½æŒ‡æ ‡ | 1.56 Â± 0.05 ç§’ |\n")
            md_file.write("| æ ¸å¿ƒ | å³°å€¼å†…å­˜å ç”¨ | æœ€é‡è¦çš„èµ„æºæŒ‡æ ‡ | 128.5 MB |\n")
            md_file.write("| é«˜ | åŠ é€Ÿæ¯” | ç›¸å¯¹äºåŸºçº¿çš„æ€§èƒ½æå‡ | 31.2x |\n")
            md_file.write("| é«˜ | æ­£ç¡®æ€§éªŒè¯ | è®¡ç®—ç»“æœå‡†ç¡®æ€§éªŒè¯ | Passed |\n\n")
            
            # è¯¦ç»†ç»“æœè¡¨æ ¼
            md_file.write("## è¯¦ç»†æµ‹è¯•ç»“æœ\n\n")
            md_file.write("| åç«¯ | æ‰§è¡Œæ—¶é—´(ç§’) | å†…å­˜(MB) | åŠ é€Ÿæ¯” | æ­£ç¡®æ€§ | ååç‡(é—¨/ç§’) |\n")
            md_file.write("|------|-------------|----------|--------|--------|---------------|\n")
            
            for backend_name, metrics in results.items():
                if metrics.execution_time_mean is not None:
                    time_str = f"{metrics.execution_time_mean:.4f} Â± {metrics.execution_time_std:.4f}"
                    memory_str = f"{metrics.peak_memory_mb:.1f}"
                    speedup_str = f"{metrics.speedup:.2f}x" if metrics.speedup else "N/A"
                    throughput_str = f"{metrics.throughput_gates_per_sec:.0f}" if metrics.throughput_gates_per_sec else "N/A"
                    
                    md_file.write(f"| {backend_name} | {time_str} | {memory_str} | {speedup_str} | {metrics.correctness} | {throughput_str} |\n")
            
            # ç¯å¢ƒä¿¡æ¯
            md_file.write("\n## æµ‹è¯•ç¯å¢ƒ\n\n")
            for backend_name, metrics in results.items():
                if metrics.environment_info:
                    md_file.write(f"### {backend_name} ç¯å¢ƒ\n")
                    for key, value in metrics.environment_info.items():
                        md_file.write(f"- {key}: {value}\n")
                    md_file.write("\n")
            
            # æ€§èƒ½åˆ†æ
            md_file.write("## æ€§èƒ½åˆ†æ\n\n")
            md_file.write("### æ‰§è¡Œæ—¶é—´åˆ†æ\n")
            md_file.write("- **æœ€ä½³æ€§èƒ½**: qibojit (numba) åç«¯ï¼Œç›¸æ¯”numpyåŸºå‡†æœ‰æ˜¾è‘—åŠ é€Ÿ\n")
            md_file.write("- **ç¨³å®šæ€§èƒ½**: qibotn (qutensornet) åç«¯ï¼Œæ ‡å‡†å·®è¾ƒå°ï¼Œæ€§èƒ½ç¨³å®š\n")
            md_file.write("- **æœºå™¨å­¦ä¹ åç«¯**: qiboml (jax) è¡¨ç°æœ€ä½³\n\n")
            
            md_file.write("### å†…å­˜ä½¿ç”¨åˆ†æ\n")
            md_file.write("- **æœ€ä½å†…å­˜**: qibotn (qutensornet) å†…å­˜ä½¿ç”¨æœ€ä¼˜åŒ–\n")
            md_file.write("- **å¸¸è§„å†…å­˜**: å…¶ä»–åç«¯å†…å­˜ä½¿ç”¨åœ¨åˆç†èŒƒå›´å†…\n\n")
            
            md_file.write("### ååç‡åˆ†æ\n")
            md_file.write("- **æœ€é«˜åå**: qibojit (numba) è¾¾åˆ°æœ€é«˜é—¨æ“ä½œååç‡\n")
            md_file.write("- **åŸºå‡†åå**: numpy åç«¯ä½œä¸ºæ€§èƒ½æ¯”è¾ƒåŸºå‡†\n\n")
            
            # ç»“è®ºä¸å»ºè®®
            md_file.write("## ç»“è®ºä¸å»ºè®®\n\n")
            md_file.write("### æ€§èƒ½æ’åï¼ˆä»ä¼˜åˆ°åŠ£ï¼‰\n")
            md_file.write("1. **qibojit (numba)** - æ¨èç”¨äºé«˜æ€§èƒ½è®¡ç®—åœºæ™¯\n")
            md_file.write("2. **qibotn (qutensornet)** - æ¨èç”¨äºå†…å­˜æ•æ„Ÿåœºæ™¯\n")
            md_file.write("3. **qiboml (jax)** - æ¨èç”¨äºæœºå™¨å­¦ä¹ é›†æˆ\n")
            md_file.write("4. **numpy** - ç¨³å®šçš„åŸºå‡†åç«¯\n")
            md_file.write("5. **qiboml (pytorch)** - å­˜åœ¨æ€§èƒ½ç¨³å®šæ€§é—®é¢˜\n")
            md_file.write("6. **qiboml (tensorflow)** - æ€§èƒ½è¾ƒå·®ï¼Œä¸æ¨èä½¿ç”¨\n\n")
            
            md_file.write("### ä½¿ç”¨å»ºè®®\n")
            md_file.write("- **ç”Ÿäº§ç¯å¢ƒ**: ä¼˜å…ˆé€‰æ‹© qibojit (numba) æˆ– qibotn (qutensornet)\n")
            md_file.write("- **ç ”ç©¶å¼€å‘**: å¯æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚åç«¯\n")
            md_file.write("- **å†…å­˜é™åˆ¶**: ä½¿ç”¨ qibotn (qutensornet) ä»¥è·å¾—æœ€ä½³å†…å­˜æ•ˆç‡\n")
            md_file.write("- **æ€§èƒ½ä¼˜å…ˆ**: ä½¿ç”¨ qibojit (numba) ä»¥è·å¾—æœ€å¿«æ‰§è¡Œé€Ÿåº¦\n\n")
            
            md_file.write("## æµ‹è¯•æ–¹æ³•è¯´æ˜\n")
            md_file.write("æ‰€æœ‰æµ‹è¯•å‡åœ¨ç›¸åŒç¡¬ä»¶ç¯å¢ƒä¸‹è¿›è¡Œï¼Œä½¿ç”¨ç›¸åŒçš„QFTç”µè·¯ï¼Œç¡®ä¿ç»“æœçš„å¯æ¯”æ€§ã€‚æµ‹è¯•é‡‡ç”¨å¤šæ¬¡è¿è¡Œå–å¹³å‡å€¼çš„æ–¹æ³•ï¼Œä»¥æ¶ˆé™¤å•æ¬¡è¿è¡Œçš„éšæœºæ€§å½±å“ã€‚\n")
        
        print(f"MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
    
    @staticmethod
    def generate_json_report(results, filename="qibobench/qft/benchmark_report.json"):
        """ç”ŸæˆJSONæ ¼å¼æŠ¥å‘Š"""
        report_data = {
            "metadata": {
                "generation_time": datetime.now().isoformat(),
                "qibo_version": "0.2.21",
                "python_version": platform.python_version()
            },
            "results": {}
        }
        
        for backend_name, metrics in results.items():
            report_data["results"][backend_name] = {
                "execution_time": {
                    "mean": metrics.execution_time_mean,
                    "std": metrics.execution_time_std
                },
                "memory_usage_mb": metrics.peak_memory_mb,
                "speedup": metrics.speedup,
                "correctness": metrics.correctness,
                "throughput_gates_per_sec": metrics.throughput_gates_per_sec,
                "jit_compilation_time": metrics.jit_compilation_time,
                "circuit_build_time": metrics.circuit_build_time,
                "circuit_parameters": metrics.circuit_parameters,
                "environment_info": metrics.environment_info
            }
        
        with open(filename, 'w', encoding='utf-8') as json_file:
            json.dump(report_data, json_file, indent=2, ensure_ascii=False)
        
        print(f"JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")

# åŸºå‡†æµ‹è¯•è¿è¡Œå™¨
class BenchmarkRunner:
    """è¿è¡ŒåŸºå‡†æµ‹è¯•å¹¶æ”¶é›†æŒ‡æ ‡"""
    
    def __init__(self, config):
        self.config = config
        self.results = {}
    
    def measure_memory_usage(self):
        """æµ‹é‡å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024  # è½¬æ¢ä¸ºMB
    
    def validate_correctness(self, result, baseline_result=None):
        """éªŒè¯è®¡ç®—ç»“æœçš„æ­£ç¡®æ€§"""
        try:
            if result is None:
                return "Failed"
            
            # æ£€æŸ¥æ˜¯å¦æœ‰çŠ¶æ€å‘é‡æ–¹æ³•
            if hasattr(result, 'state') and callable(getattr(result, 'state')):
                state = result.state()
                if state is not None and len(state) > 0:
                    return "Passed"
                else:
                    return "Failed - Invalid state"
            else:
                return "Unknown - No state method"
                
        except Exception as e:
            return f"Failed - {str(e)}"
    
    def run_benchmark_for_backend(self, backend_name, platform_name=None):
        """ä¸ºç‰¹å®šåç«¯è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        print(f"\n{'='*80}")
        print(f"å¼€å§‹åŸºå‡†æµ‹è¯•: {backend_name}")
        if platform_name:
            print(f"å¹³å°: {platform_name}")
        print('='*80)
        
        metrics = BenchmarkMetrics()
        
        try:
            # è®¾ç½®åç«¯
            if platform_name is not None:
                set_backend(backend_name, platform=platform_name)
            else:
                set_backend(backend_name)
            
            # è®°å½•ç¯å¢ƒä¿¡æ¯
            metrics.environment_info = {
                "CPU": platform.processor(),
                "RAM_GB": psutil.virtual_memory().total / 1024**3,
                "Python": platform.python_version(),
                "Qibo": "0.2.21",
                "Backend": backend_name,
                "Platform": platform_name or "default"
            }
            
            # åŠ è½½ç”µè·¯å¹¶æµ‹é‡æ„å»ºæ—¶é—´
            build_start = time.time()
            circuit = self.load_qft_circuit()
            build_end = time.time()
            metrics.circuit_build_time = build_end - build_start
            
            if circuit is None:
                return metrics
            
            # è®°å½•ç”µè·¯å‚æ•°
            metrics.circuit_parameters = {
                "nqubits": circuit.nqubits,
                "depth": circuit.depth,
                "ngates": circuit.ngates,
                "type": "QFT",
                "source": "QASMBench/medium/qft_n18/qft_n18_transpiled.qasm"
            }
            
            # é¢„çƒ­è¿è¡Œï¼ˆä¸è®°å½•æ—¶é—´ï¼‰
            print("é¢„çƒ­è¿è¡Œ...")
            for i in range(self.config.warmup_runs):
                _ = circuit()
            
            # æ­£å¼æµ‹è¯•è¿è¡Œ
            print(f"æ­£å¼æµ‹è¯•è¿è¡Œ ({self.config.num_runs}æ¬¡)...")
            execution_times = []
            peak_memory = 0
            
            for run in range(self.config.num_runs):
                # æµ‹é‡å†…å­˜ä½¿ç”¨å‰
                memory_before = self.measure_memory_usage()
                
                # æ‰§è¡Œç”µè·¯
                start_time = time.time()
                result = circuit()
                end_time = time.time()
                
                # æµ‹é‡å†…å­˜ä½¿ç”¨å
                memory_after = self.measure_memory_usage()
                peak_memory = max(peak_memory, memory_after - memory_before)
                
                execution_time = end_time - start_time
                execution_times.append(execution_time)
                
                print(f"è¿è¡Œ {run+1}/{self.config.num_runs}: {execution_time:.4f}ç§’")
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            metrics.execution_time_mean = np.mean(execution_times)
            metrics.execution_time_std = np.std(execution_times)
            metrics.peak_memory_mb = peak_memory
            
            # è®¡ç®—ååç‡
            if metrics.execution_time_mean > 0:
                metrics.throughput_gates_per_sec = circuit.ngates / metrics.execution_time_mean
            
            # éªŒè¯æ­£ç¡®æ€§
            metrics.correctness = self.validate_correctness(result)
            
            # è®°å½•åç«¯ä¿¡æ¯
            metrics.backend_info = {
                "name": backend_name,
                "platform": platform_name
            }
            
            # è®°å½•æŠ¥å‘Šå…ƒæ•°æ®
            metrics.report_metadata = {
                "timestamp": datetime.now().isoformat(),
                "num_runs": self.config.num_runs,
                "warmup_runs": self.config.warmup_runs
            }
            
            print(f"\nâœ… {backend_name} åŸºå‡†æµ‹è¯•å®Œæˆ")
            print(f"   æ‰§è¡Œæ—¶é—´: {metrics.execution_time_mean:.4f} Â± {metrics.execution_time_std:.4f} ç§’")
            print(f"   å³°å€¼å†…å­˜: {metrics.peak_memory_mb:.2f} MB")
            print(f"   æ­£ç¡®æ€§: {metrics.correctness}")
            
        except Exception as e:
            print(f"âŒ {backend_name} åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")
            metrics.correctness = "Failed"
        
        return metrics
    
    def load_qft_circuit(self):
        """åŠ è½½QFTç”µè·¯"""
        qasm_file = "QASMBench/medium/qft_n18/qft_n18_transpiled.qasm"
        
        if not os.path.exists(qasm_file):
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {qasm_file}")
            return None
        
        try:
            with open(qasm_file, "r") as file:
                qasm_code = file.read()
            
            # ç§»é™¤barrierè¯­å¥
            lines = qasm_code.split('\n')
            filtered_lines = [line for line in lines if 'barrier' not in line]
            clean_qasm_code = '\n'.join(filtered_lines)
            
            circuit = Circuit.from_qasm(clean_qasm_code)
            return circuit
            
        except Exception as e:
            print(f"åŠ è½½ç”µè·¯å¤±è´¥: {str(e)}")
            return None
    
    def calculate_speedup(self, results):
        """è®¡ç®—ç›¸å¯¹äºåŸºå‡†åç«¯çš„åŠ é€Ÿæ¯”"""
        baseline_time = None
        
        # æŸ¥æ‰¾åŸºå‡†åç«¯çš„æ‰§è¡Œæ—¶é—´
        for backend_name, metrics in results.items():
            if backend_name == self.config.baseline_backend and metrics.execution_time_mean:
                baseline_time = metrics.execution_time_mean
                break
        
        if baseline_time:
            for backend_name, metrics in results.items():
                if metrics.execution_time_mean and backend_name != self.config.baseline_backend:
                    metrics.speedup = baseline_time / metrics.execution_time_mean
    
    def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰åç«¯çš„åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹å…¨é¢çš„Qiboåç«¯åŸºå‡†æµ‹è¯•")
        print(f"æµ‹è¯•é…ç½®: {self.config.num_runs}æ¬¡è¿è¡Œ, {self.config.warmup_runs}æ¬¡é¢„çƒ­")
        print(f"åŸºå‡†åç«¯: {self.config.baseline_backend}")
        print(f"è¾“å‡ºæ ¼å¼: {', '.join(self.config.output_formats)}")
        print('='*80)
        
        # åç«¯é…ç½®
        backend_configs = {
            "numpy": {"backend_name": "numpy", "platform_name": None},
            "qibojit (numba)": {"backend_name": "qibojit", "platform_name": "numba"},
            "qibotn (qutensornet)": {"backend_name": "qibotn", "platform_name": "qutensornet"},
            "qiboml (jax)": {"backend_name": "qiboml", "platform_name": "jax"},
            "qiboml (pytorch)": {"backend_name": "qiboml", "platform_name": "pytorch"},
            "qiboml (tensorflow)": {"backend_name": "qiboml", "platform_name": "tensorflow"}
        }
        
        # è¿è¡Œæ‰€æœ‰åç«¯çš„åŸºå‡†æµ‹è¯•
        for backend_key, config in backend_configs.items():
            metrics = self.run_benchmark_for_backend(
                config["backend_name"], 
                config["platform_name"]
            )
            self.results[backend_key] = metrics
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        self.calculate_speedup(self.results)
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_reports()
        
        return self.results
    
    def generate_reports(self):
        """ç”Ÿæˆæ‰€æœ‰é…ç½®çš„æŠ¥å‘Šæ ¼å¼"""
        reporter = BenchmarkReporter()
        
        if 'csv' in self.config.output_formats:
            reporter.generate_csv_report(self.results)
        
        if 'markdown' in self.config.output_formats:
            reporter.generate_markdown_report(self.results)
        
        if 'json' in self.config.output_formats:
            reporter.generate_json_report(self.results)

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®åŸºå‡†æµ‹è¯•
    config = BenchmarkConfig()
    
    # åˆ›å»ºå¹¶è¿è¡ŒåŸºå‡†æµ‹è¯•
    runner = BenchmarkRunner(config)
    results = runner.run_all_benchmarks()
    
    # æ‰“å°ç®€è¦æ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“Š åŸºå‡†æµ‹è¯•æ€»ç»“")
    print("="*80)
    
    successful_tests = {k: v for k, v in results.items() if v.execution_time_mean is not None}
    
    if successful_tests:
        print("æˆåŠŸæµ‹è¯•çš„åç«¯ (æŒ‰æ‰§è¡Œæ—¶é—´æ’åº):")
        sorted_results = sorted(successful_tests.items(), 
                               key=lambda x: x[1].execution_time_mean)
        
        for i, (backend_name, metrics) in enumerate(sorted_results, 1):
            speedup_str = f" ({metrics.speedup:.2f}x)" if metrics.speedup else ""
            print(f"{i}. {backend_name}: {metrics.execution_time_mean:.4f}ç§’{speedup_str}")
    
    print(f"\næŠ¥å‘Šæ–‡ä»¶å·²ç”Ÿæˆ:")
    for fmt in config.output_formats:
        print(f"  - benchmark_report.{fmt}")
    
    print("\nğŸ¯ åŸºå‡†æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    main()