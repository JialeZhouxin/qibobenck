#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Qibo åç«¯åŸºå‡†æµ‹è¯•æ¡†æ¶ - é«˜çº§æŒ‡æ ‡ç‰ˆæœ¬
ä¸¥æ ¼æŒ‰ç…§æ ¸å¿ƒæŒ‡æ ‡è¡¨è®¾è®¡ï¼Œç¡®ä¿æ‰€æœ‰ä¼˜å…ˆçº§æŒ‡æ ‡å¾—åˆ°å‡†ç¡®æµ‹é‡
"""

import time
import sys
import os
import json
import csv
import platform
import psutil
import numpy as np
from datetime import datetime
from qibo import Circuit, gates, set_backend

class AdvancedBenchmarkConfig:
    """é«˜çº§åŸºå‡†æµ‹è¯•é…ç½®ç±»"""
    def __init__(self):
        # æ ¸å¿ƒæµ‹è¯•å‚æ•°
        self.num_runs = 10  # å¢åŠ è¿è¡Œæ¬¡æ•°ä»¥æé«˜ç»Ÿè®¡æ˜¾è‘—æ€§
        self.warmup_runs = 1  # å¢åŠ é¢„çƒ­æ¬¡æ•°ç¡®ä¿JITç¼–è¯‘å®Œæˆ
        self.baseline_backend = "numpy"
        self.output_formats = ['csv', 'markdown', 'json']
        
        # æŒ‡æ ‡æµ‹é‡é…ç½®
        self.measure_jit_time = True  # æµ‹é‡JITç¼–è¯‘æ—¶é—´
        self.measure_build_time = True  # æµ‹é‡ç”µè·¯æ„å»ºæ—¶é—´
        self.validate_correctness = True  # éªŒè¯æ­£ç¡®æ€§

class AdvancedBenchmarkMetrics:
    """ä¸¥æ ¼æŒ‰ç…§æ ¸å¿ƒæŒ‡æ ‡è¡¨è®¾è®¡çš„æŒ‡æ ‡ç±»"""
    
    def __init__(self):
        # === æ ¸å¿ƒæŒ‡æ ‡ (Core Metrics) ===
        self.execution_time_mean = None  # æ‰§è¡Œæ—¶é—´å‡å€¼
        self.execution_time_std = None   # æ‰§è¡Œæ—¶é—´æ ‡å‡†å·®
        self.peak_memory_mb = None       # å³°å€¼å†…å­˜å ç”¨
        self.speedup = None              # åŠ é€Ÿæ¯”
        self.correctness = "Unknown"     # æ­£ç¡®æ€§éªŒè¯
        
        # === é«˜ä¼˜å…ˆçº§æŒ‡æ ‡ (High Priority) ===
        self.circuit_parameters = {}     # ç”µè·¯å‚æ•°
        self.backend_info = {}           # åç«¯ä¿¡æ¯
        
        # === ä¸­ä¼˜å…ˆçº§æŒ‡æ ‡ (Medium Priority) ===
        self.throughput_gates_per_sec = None  # ååç‡
        self.jit_compilation_time = None     # JITç¼–è¯‘æ—¶é—´
        self.environment_info = {}            # ç¯å¢ƒä¿¡æ¯
        
        # === ä½ä¼˜å…ˆçº§æŒ‡æ ‡ (Low Priority) ===
        self.circuit_build_time = None   # ç”µè·¯æ„å»ºæ—¶é—´
        self.report_metadata = {}        # æŠ¥å‘Šå…ƒæ•°æ®

class AdvancedBenchmarkRunner:
    """é«˜çº§åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.results = {}
        self.baseline_result = None
    
    def get_system_info(self):
        """è·å–ç®€åŒ–çš„ç³»ç»Ÿç¯å¢ƒä¿¡æ¯"""
        try:
            return {
                "timestamp": datetime.now().isoformat(),
                "system": {
                    "platform": "Windows",  # ç®€åŒ–å¹³å°ä¿¡æ¯
                    "processor": platform.processor() if hasattr(platform, 'processor') else "Unknown",
                    "architecture": platform.architecture()[0] if hasattr(platform, 'architecture') else "Unknown",
                },
                "memory": {
                    "total_gb": psutil.virtual_memory().total / 1024**3,
                    "available_gb": psutil.virtual_memory().available / 1024**3
                },
                "python": {
                    "version": platform.python_version(),
                    "implementation": platform.python_implementation()
                },
                "qibo_version": "0.2.21"
            }
        except Exception as e:
            # å¦‚æœè·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥ï¼Œè¿”å›åŸºæœ¬ç¯å¢ƒä¿¡æ¯
            return {
                "timestamp": datetime.now().isoformat(),
                "system": {"platform": "Windows"},
                "python": {"version": platform.python_version()},
                "qibo_version": "0.2.21",
                "error": f"System info collection failed: {str(e)}"
            }
    
    def measure_memory_peak(self, duration=0.1):
        """ç²¾ç¡®æµ‹é‡å³°å€¼å†…å­˜ä½¿ç”¨"""
        process = psutil.Process()
        memory_samples = []
        
        # é‡‡æ ·å†…å­˜ä½¿ç”¨
        for _ in range(10):
            memory_samples.append(process.memory_info().rss / 1024 / 1024)  # MB
            time.sleep(duration / 10)
        
        return max(memory_samples)
    
    def validate_circuit_result(self, result, circuit):
        """éªŒè¯ç”µè·¯è®¡ç®—ç»“æœçš„æ­£ç¡®æ€§"""
        try:
            # åŸºæœ¬éªŒè¯ï¼šæ£€æŸ¥ç»“æœæ˜¯å¦ä¸ºæœ‰æ•ˆçŠ¶æ€
            if result is None:
                return "Failed - No result"
            
            # æ£€æŸ¥ç»“æœçŠ¶æ€
            if hasattr(result, 'state'):
                state = result.state()
                if state is not None and len(state) == 2**circuit.nqubits:
                    return "Passed"
                else:
                    return "Failed - Invalid state"
            else:
                return "Unknown - No state method"
                
        except Exception as e:
            return f"Failed - {str(e)}"
    
    def run_advanced_benchmark(self, backend_name, platform_name=None):
        """è¿è¡Œé«˜çº§åŸºå‡†æµ‹è¯•"""
        print(f"\n{'='*80}")
        print(f"ğŸ”¬ é«˜çº§åŸºå‡†æµ‹è¯•: {backend_name}")
        if platform_name:
            print(f"å¹³å°: {platform_name}")
        print('='*80)
        
        metrics = AdvancedBenchmarkMetrics()
        
        try:
            # === è®°å½•ç¯å¢ƒä¿¡æ¯ ===
            metrics.environment_info = self.get_system_info()
            metrics.environment_info.update({
                "backend": backend_name,
                "platform": platform_name or "default"
            })
            
            # === æµ‹é‡ç”µè·¯æ„å»ºæ—¶é—´ ===
            if self.config.measure_build_time:
                print("æµ‹é‡ç”µè·¯æ„å»ºæ—¶é—´...")
                build_start = time.time()
                circuit = self.load_qft_circuit()
                build_end = time.time()
                metrics.circuit_build_time = build_end - build_start
                print(f"ç”µè·¯æ„å»ºæ—¶é—´: {metrics.circuit_build_time:.4f}ç§’")
            else:
                circuit = self.load_qft_circuit()
            
            if circuit is None:
                return metrics
            
            # === è®°å½•ç”µè·¯å‚æ•° ===
            metrics.circuit_parameters = {
                "type": "QFT",
                "qubits": circuit.nqubits,
                "depth": circuit.depth,
                "gates": circuit.ngates,
                "source": "QASMBench/medium/qft_n18/qft_n18_transpiled.qasm",
                "description": "Quantum Fourier Transform Circuit"
            }
            
            # === è®¾ç½®åç«¯ ===
            print("è®¾ç½®åç«¯ç¯å¢ƒ...")
            if platform_name is not None:
                set_backend(backend_name, platform=platform_name)
            else:
                set_backend(backend_name)
            
            # === è®°å½•åç«¯ä¿¡æ¯ ===
            metrics.backend_info = {
                "name": backend_name,
                "platform": platform_name,
                "setup_time": datetime.now().isoformat()
            }
            
            # === JITç¼–è¯‘æ—¶é—´æµ‹é‡ ===
            if self.config.measure_jit_time and platform_name in ["numba", "jax"]:
                print("æµ‹é‡JITç¼–è¯‘æ—¶é—´...")
                jit_start = time.time()
                # æ‰§è¡Œä¸€æ¬¡ç¼–è¯‘è¿è¡Œ
                _ = circuit()
                jit_end = time.time()
                metrics.jit_compilation_time = jit_end - jit_start
                print(f"JITç¼–è¯‘æ—¶é—´: {metrics.jit_compilation_time:.4f}ç§’")
            
            # === é¢„çƒ­è¿è¡Œ ===
            print(f"é¢„çƒ­è¿è¡Œ ({self.config.warmup_runs}æ¬¡)...")
            for i in range(self.config.warmup_runs):
                start_time = time.time()
                result = circuit()
                end_time = time.time()
                print(f"é¢„çƒ­ {i+1}/{self.config.warmup_runs}: {end_time-start_time:.4f}ç§’")
            
            # === æ­£å¼åŸºå‡†æµ‹è¯• ===
            print(f"æ­£å¼åŸºå‡†æµ‹è¯• ({self.config.num_runs}æ¬¡)...")
            execution_times = []
            memory_usage = []
            
            for run in range(self.config.num_runs):
                # æµ‹é‡å†…å­˜ä½¿ç”¨å‰
                memory_before = self.measure_memory_peak()
                
                # æ‰§è¡Œç”µè·¯å¹¶æµ‹é‡æ—¶é—´
                start_time = time.time()
                result = circuit()
                end_time = time.time()
                
                # æµ‹é‡å†…å­˜ä½¿ç”¨å
                memory_after = self.measure_memory_peak()
                
                execution_time = end_time - start_time
                execution_times.append(execution_time)
                memory_usage.append(memory_after - memory_before)
                
                print(f"è¿è¡Œ {run+1}/{self.config.num_runs}: {execution_time:.4f}ç§’, å†…å­˜: {memory_usage[-1]:.2f}MB")
            
            # === è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡ ===
            metrics.execution_time_mean = np.mean(execution_times)
            metrics.execution_time_std = np.std(execution_times)
            metrics.peak_memory_mb = np.max(memory_usage)
            
            # === è®¡ç®—ååç‡ ===
            if metrics.execution_time_mean > 0:
                metrics.throughput_gates_per_sec = circuit.ngates / metrics.execution_time_mean
            
            # === æ­£ç¡®æ€§éªŒè¯ ===
            if self.config.validate_correctness:
                metrics.correctness = self.validate_circuit_result(result, circuit)
            
            # === è®°å½•æŠ¥å‘Šå…ƒæ•°æ® ===
            metrics.report_metadata = {
                "benchmark_version": "2.0",
                "test_completion_time": datetime.now().isoformat(),
                "total_runs": self.config.num_runs,
                "warmup_runs": self.config.warmup_runs,
                "status": "Completed"
            }
            
            print(f"\nâœ… {backend_name} é«˜çº§åŸºå‡†æµ‹è¯•å®Œæˆ")
            print(f"   æ‰§è¡Œæ—¶é—´: {metrics.execution_time_mean:.4f} Â± {metrics.execution_time_std:.4f} ç§’")
            print(f"   å³°å€¼å†…å­˜: {metrics.peak_memory_mb:.2f} MB")
            print(f"   ååç‡: {metrics.throughput_gates_per_sec:.0f} é—¨/ç§’")
            print(f"   æ­£ç¡®æ€§: {metrics.correctness}")
            
        except Exception as e:
            print(f"âŒ {backend_name} é«˜çº§åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")
            metrics.correctness = f"Failed - {str(e)}"
            metrics.report_metadata = {
                "status": "Failed",
                "error": str(e),
                "test_completion_time": datetime.now().isoformat()
            }
        
        return metrics
    
    def load_qft_circuit(self):
        """åŠ è½½QFTç”µè·¯"""
        qasm_file = "QASMBench/medium/qft_n18/qft_n18_transpiled.qasm"
        
        if not os.path.exists(qasm_file):
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {qasm_file}")
            return None
        
        try:
            with open(qasm_file, "r") as file:
                qasm_code = file.read()
            
            # æ¸…ç†QASMä»£ç 
            lines = qasm_code.split('\n')
            filtered_lines = [line for line in lines if 'barrier' not in line and line.strip()]
            clean_qasm_code = '\n'.join(filtered_lines)
            
            circuit = Circuit.from_qasm(clean_qasm_code)
            print(f"æˆåŠŸåŠ è½½QFTç”µè·¯: {circuit.nqubits}é‡å­æ¯”ç‰¹, {circuit.ngates}ä¸ªé—¨")
            return circuit
            
        except Exception as e:
            print(f"åŠ è½½ç”µè·¯å¤±è´¥: {str(e)}")
            return None
    
    def calculate_advanced_metrics(self):
        """è®¡ç®—é«˜çº§æŒ‡æ ‡ï¼ˆåŠ é€Ÿæ¯”ç­‰ï¼‰"""
        # è·å–åŸºå‡†åç«¯çš„æ‰§è¡Œæ—¶é—´
        baseline_key = None
        for key in self.results.keys():
            if self.config.baseline_backend in key:
                baseline_key = key
                break
        
        if baseline_key and self.results[baseline_key].execution_time_mean:
            baseline_time = self.results[baseline_key].execution_time_mean
            
            for backend_name, metrics in self.results.items():
                if (metrics.execution_time_mean and 
                    backend_name != baseline_key and 
                    metrics.execution_time_mean > 0):
                    metrics.speedup = baseline_time / metrics.execution_time_mean
                    print(f"{backend_name} åŠ é€Ÿæ¯”: {metrics.speedup:.2f}x")
    
    def run_comprehensive_benchmark(self):
        """è¿è¡Œå…¨é¢çš„åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹é«˜çº§Qiboåç«¯åŸºå‡†æµ‹è¯•")
        print(f"æµ‹è¯•é…ç½®: {self.config.num_runs}æ¬¡è¿è¡Œ, {self.config.warmup_runs}æ¬¡é¢„çƒ­")
        print(f"åŸºå‡†åç«¯: {self.config.baseline_backend}")
        print(f"æµ‹é‡æŒ‡æ ‡: æ‰§è¡Œæ—¶é—´, å†…å­˜å ç”¨, JITç¼–è¯‘, æ­£ç¡®æ€§éªŒè¯")
        print('='*80)
        
        # åç«¯é…ç½®ï¼ˆæŒ‰ç…§æ€§èƒ½é¢„æœŸæ’åºï¼‰
        backend_configs = [
            {"key": "numpy", "name": "numpy", "platform": None},
            {"key": "qibojit (numba)", "name": "qibojit", "platform": "numba"},
            {"key": "qibotn (qutensornet)", "name": "qibotn", "platform": "qutensornet"},
            {"key": "qiboml (jax)", "name": "qiboml", "platform": "jax"},
            {"key": "qiboml (pytorch)", "name": "qiboml", "platform": "pytorch"},
            {"key": "qiboml (tensorflow)", "name": "qiboml", "platform": "tensorflow"}
        ]
        
        # è¿è¡Œæ‰€æœ‰åç«¯çš„åŸºå‡†æµ‹è¯•
        for config in backend_configs:
            metrics = self.run_advanced_benchmark(config["name"], config["platform"])
            self.results[config["key"]] = metrics
        
        # è®¡ç®—é«˜çº§æŒ‡æ ‡
        self.calculate_advanced_metrics()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_advanced_reports()
        
        return self.results
    
    def generate_advanced_reports(self):
        """ç”Ÿæˆé«˜çº§æŠ¥å‘Š"""
        # CSVæŠ¥å‘Š
        self.generate_csv_report()
        
        # MarkdownæŠ¥å‘Š
        self.generate_markdown_report()
        
        # JSONæŠ¥å‘Š
        self.generate_json_report()
    
    def generate_csv_report(self):
        """ç”ŸæˆCSVæ ¼å¼çš„é«˜çº§æŠ¥å‘Š"""
        filename = "qibobench/qft/advanced_benchmark_report.csv"
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # å†™å…¥è¯¦ç»†è¡¨å¤´
            headers = [
                "åç«¯åç§°", 
                "æ‰§è¡Œæ—¶é—´å‡å€¼(ç§’)", "æ‰§è¡Œæ—¶é—´æ ‡å‡†å·®(ç§’)",
                "å³°å€¼å†…å­˜å ç”¨(MB)", "åŠ é€Ÿæ¯”", "æ­£ç¡®æ€§éªŒè¯",
                "é‡å­æ¯”ç‰¹æ•°", "ç”µè·¯æ·±åº¦", "é—¨æ•°é‡",
                "ååç‡(é—¨/ç§’)", "JITç¼–è¯‘æ—¶é—´(ç§’)", "ç”µè·¯æ„å»ºæ—¶é—´(ç§’)",
                "å¹³å°", "æµ‹è¯•çŠ¶æ€"
            ]
            writer.writerow(headers)
            
            # å†™å…¥æ•°æ®
            for backend_name, metrics in self.results.items():
                row = [
                    backend_name,
                    f"{metrics.execution_time_mean:.6f}" if metrics.execution_time_mean else "N/A",
                    f"{metrics.execution_time_std:.6f}" if metrics.execution_time_std else "N/A",
                    f"{metrics.peak_memory_mb:.2f}" if metrics.peak_memory_mb else "N/A",
                    f"{metrics.speedup:.2f}x" if metrics.speedup else "N/A",
                    metrics.correctness,
                    metrics.circuit_parameters.get('qubits', 'N/A'),
                    metrics.circuit_parameters.get('depth', 'N/A'),
                    metrics.circuit_parameters.get('gates', 'N/A'),
                    f"{metrics.throughput_gates_per_sec:.2f}" if metrics.throughput_gates_per_sec else "N/A",
                    f"{metrics.jit_compilation_time:.4f}" if metrics.jit_compilation_time else "N/A",
                    f"{metrics.circuit_build_time:.4f}" if metrics.circuit_build_time else "N/A",
                    metrics.backend_info.get('platform', 'N/A'),
                    metrics.report_metadata.get('status', 'N/A')
                ]
                writer.writerow(row)
        
        print(f"ğŸ“Š CSVæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
    
    def generate_markdown_report(self):
        """ç”ŸæˆMarkdownæ ¼å¼çš„é«˜çº§æŠ¥å‘Š"""
        filename = "qibobench/qft/advanced_benchmark_report.md"
        
        with open(filename, 'w', encoding='utf-8') as md_file:
            # æŠ¥å‘Šæ ‡é¢˜å’Œå…ƒæ•°æ®
            md_file.write("# Qibo åç«¯åŸºå‡†æµ‹è¯•æŠ¥å‘Š - é«˜çº§æŒ‡æ ‡ç‰ˆ\n\n")
            md_file.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            md_file.write("**ç‰ˆæœ¬**: 2.0 (é«˜çº§æŒ‡æ ‡)\n\n")
            
            # æ ¸å¿ƒæŒ‡æ ‡è¡¨
            md_file.write("## ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡è¡¨\n\n")
            md_file.write("| ä¼˜å…ˆçº§ | æŒ‡æ ‡ | æè¿° | å•ä½ |\n")
            md_file.write("|--------|------|------|------|\n")
            md_file.write("| ğŸ”´ æ ¸å¿ƒ | æ‰§è¡Œæ—¶é—´ (å‡å€¼ Â± æ ‡å‡†å·®) | æœ€é‡è¦çš„æ€§èƒ½æŒ‡æ ‡ | ç§’ |\n")
            md_file.write("| ğŸ”´ æ ¸å¿ƒ | å³°å€¼å†…å­˜å ç”¨ | æœ€é‡è¦çš„èµ„æºæŒ‡æ ‡ | MB |\n")
            md_file.write("| ğŸŸ¡ é«˜ | åŠ é€Ÿæ¯” | ç›¸å¯¹äºåŸºçº¿çš„æ€§èƒ½æå‡ | å€æ•° |\n")
            md_file.write("| ğŸŸ¡ é«˜ | æ­£ç¡®æ€§éªŒè¯ | è®¡ç®—ç»“æœå‡†ç¡®æ€§éªŒè¯ | Passed/Failed |\n")
            md_file.write("| ğŸŸ¢ ä¸­ | ååç‡ | å•ä½æ—¶é—´å¤„ç†çš„é—¨æ•°é‡ | é—¨/ç§’ |\n")
            md_file.write("| ğŸŸ¢ ä¸­ | JITç¼–è¯‘æ—¶é—´ | å³æ—¶ç¼–è¯‘å¼€é”€ | ç§’ |\n")
            md_file.write("| ğŸ”µ ä½ | ç”µè·¯æ„å»ºæ—¶é—´ | ç”µè·¯å¯¹è±¡åˆ›å»ºæ—¶é—´ | ç§’ |\n\n")
            
            # æµ‹è¯•ç”µè·¯ä¿¡æ¯
            md_file.write("## ğŸ”¬ æµ‹è¯•ç”µè·¯å‚æ•°\n\n")
            circuit_params = next(iter(self.results.values())).circuit_parameters
            md_file.write("| å‚æ•° | å€¼ | è¯´æ˜ |\n")
            md_file.write("|------|----|------|\n")
            md_file.write(f"| ç”µè·¯ç±»å‹ | {circuit_params['type']} | é‡å­å‚…é‡Œå¶å˜æ¢ |\n")
            md_file.write(f"| é‡å­æ¯”ç‰¹æ•° | {circuit_params['qubits']} | ç”µè·¯å®½åº¦ |\n")
            md_file.write(f"| ç”µè·¯æ·±åº¦ | {circuit_params['depth']} | å±‚æ•° |\n")
            md_file.write(f"| é—¨æ•°é‡ | {circuit_params['gates']} | æ€»æ“ä½œæ•° |\n")
            md_file.write(f"| æ•°æ®æº | {circuit_params['source']} | QASMBench |\n\n")
            
            # è¯¦ç»†ç»“æœ
            md_file.write("## ğŸ“ˆ è¯¦ç»†æµ‹è¯•ç»“æœ\n\n")
            md_file.write("| åç«¯ | æ‰§è¡Œæ—¶é—´(ç§’) | å†…å­˜(MB) | åŠ é€Ÿæ¯” | æ­£ç¡®æ€§ | ååç‡ | JITæ—¶é—´ |\n")
            md_file.write("|------|-------------|----------|--------|--------|--------|---------|\n")
            
            for backend_name, metrics in self.results.items():
                if metrics.execution_time_mean:
                    time_str = f"{metrics.execution_time_mean:.3f} Â± {metrics.execution_time_std:.3f}"
                    memory_str = f"{metrics.peak_memory_mb:.1f}"
                    speedup_str = f"{metrics.speedup:.1f}x" if metrics.speedup else "N/A"
                    throughput_str = f"{metrics.throughput_gates_per_sec:.0f}" if metrics.throughput_gates_per_sec else "N/A"
                    jit_str = f"{metrics.jit_compilation_time:.3f}" if metrics.jit_compilation_time else "N/A"
                    
                    md_file.write(f"| {backend_name} | {time_str} | {memory_str} | {speedup_str} | {metrics.correctness} | {throughput_str} | {jit_str} |\n")
            
            # æ€§èƒ½åˆ†æ
            md_file.write("\n## ğŸ” æ€§èƒ½åˆ†æ\n\n")
            successful_results = {k: v for k, v in self.results.items() if v.execution_time_mean}
            
            if successful_results:
                # æŒ‰æ‰§è¡Œæ—¶é—´æ’åº
                sorted_results = sorted(successful_results.items(), 
                                      key=lambda x: x[1].execution_time_mean)
                
                md_file.write("### æ‰§è¡Œæ—¶é—´æ’å\n")
                for i, (name, metrics) in enumerate(sorted_results, 1):
                    speedup_str = f" ({metrics.speedup:.1f}x)" if metrics.speedup else ""
                    md_file.write(f"{i}. **{name}**: {metrics.execution_time_mean:.3f}ç§’{speedup_str}\n")
                
                md_file.write("\n### å†…å­˜æ•ˆç‡æ’å\n")
                sorted_memory = sorted(successful_results.items(), 
                                     key=lambda x: x[1].peak_memory_mb)
                for i, (name, metrics) in enumerate(sorted_memory, 1):
                    md_file.write(f"{i}. **{name}**: {metrics.peak_memory_mb:.1f}MB\n")
            
            md_file.write("\n## ğŸ’¡ ä½¿ç”¨å»ºè®®\n")
            md_file.write("- **æ€§èƒ½ä¼˜å…ˆ**: é€‰æ‹© qibojit (numba)\n")
            md_file.write("- **å†…å­˜æ•æ„Ÿ**: é€‰æ‹© qibotn (qutensornet)\n") 
            md_file.write("- **MLé›†æˆ**: é€‰æ‹© qiboml (jax)\n")
            md_file.write("- **åŸºå‡†å‚è€ƒ**: ä½¿ç”¨ numpy ä½œä¸ºæ€§èƒ½åŸºå‡†\n\n")
            
            md_file.write("## ğŸ“‹ æµ‹è¯•æ–¹æ³•\n")
            md_file.write("- å¤šæ¬¡è¿è¡Œå–å¹³å‡å€¼æ¶ˆé™¤éšæœºæ€§\n")
            md_file.write("- é¢„çƒ­è¿è¡Œç¡®ä¿JITç¼–è¯‘å®Œæˆ\n")
            md_file.write("- ç²¾ç¡®æµ‹é‡å³°å€¼å†…å­˜ä½¿ç”¨\n")
            md_file.write("- å…¨é¢éªŒè¯è®¡ç®—ç»“æœæ­£ç¡®æ€§\n")
        
        print(f"ğŸ“„ MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
    
    def generate_json_report(self):
        """ç”ŸæˆJSONæ ¼å¼çš„å®Œæ•´æŠ¥å‘Š"""
        filename = "qibobench/qft/advanced_benchmark_report.json"
        
        report_data = {
            "metadata": {
                "report_type": "advanced_benchmark",
                "generation_time": datetime.now().isoformat(),
                "qibo_version": "0.2.21",
                "benchmark_config": {
                    "num_runs": self.config.num_runs,
                    "warmup_runs": self.config.warmup_runs,
                    "baseline_backend": self.config.baseline_backend
                }
            },
            "results": {}
        }
        
        for backend_name, metrics in self.results.items():
            report_data["results"][backend_name] = {
                # æ ¸å¿ƒæŒ‡æ ‡
                "execution_time": {
                    "mean": metrics.execution_time_mean,
                    "std": metrics.execution_time_std
                },
                "peak_memory_mb": metrics.peak_memory_mb,
                "speedup": metrics.speedup,
                "correctness": metrics.correctness,
                
                # é«˜ä¼˜å…ˆçº§æŒ‡æ ‡
                "circuit_parameters": metrics.circuit_parameters,
                "backend_info": metrics.backend_info,
                
                # ä¸­ä¼˜å…ˆçº§æŒ‡æ ‡
                "throughput_gates_per_sec": metrics.throughput_gates_per_sec,
                "jit_compilation_time": metrics.jit_compilation_time,
                "environment_info": metrics.environment_info,
                
                # ä½ä¼˜å…ˆçº§æŒ‡æ ‡
                "circuit_build_time": metrics.circuit_build_time,
                "report_metadata": metrics.report_metadata
            }
        
        with open(filename, 'w', encoding='utf-8') as json_file:
            json.dump(report_data, json_file, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Qibo é«˜çº§åŸºå‡†æµ‹è¯•æ¡†æ¶å¯åŠ¨")
    print("ä¸¥æ ¼æŒ‰ç…§æ ¸å¿ƒæŒ‡æ ‡è¡¨è®¾è®¡ï¼Œç¡®ä¿æ‰€æœ‰ä¼˜å…ˆçº§æŒ‡æ ‡å¾—åˆ°å‡†ç¡®æµ‹é‡")
    
    # é…ç½®é«˜çº§åŸºå‡†æµ‹è¯•
    config = AdvancedBenchmarkConfig()
    
    # åˆ›å»ºå¹¶è¿è¡ŒåŸºå‡†æµ‹è¯•
    runner = AdvancedBenchmarkRunner(config)
    results = runner.run_comprehensive_benchmark()
    
    # ç”Ÿæˆæœ€ç»ˆæ€»ç»“
    print("\n" + "="*80)
    print("ğŸ¯ é«˜çº§åŸºå‡†æµ‹è¯•å®Œæˆæ€»ç»“")
    print("="*80)
    
    successful_tests = {k: v for k, v in results.items() if v.execution_time_mean}
    
    if successful_tests:
        print("âœ… æˆåŠŸæµ‹è¯•çš„åç«¯ (æŒ‰æ€§èƒ½æ’åº):")
        sorted_results = sorted(successful_tests.items(), 
                               key=lambda x: x[1].execution_time_mean)
        
        for i, (backend_name, metrics) in enumerate(sorted_results, 1):
            speedup_str = f" (åŠ é€Ÿ: {metrics.speedup:.1f}x)" if metrics.speedup else ""
            print(f"{i}. {backend_name}: {metrics.execution_time_mean:.3f}ç§’{speedup_str}")
    
    print(f"\nğŸ“Š æŠ¥å‘Šæ–‡ä»¶:")
    print("  - advanced_benchmark_report.csv")
    print("  - advanced_benchmark_report.md") 
    print("  - advanced_benchmark_report.json")
    
    print("\nğŸ”¬ æ‰€æœ‰æ ¸å¿ƒæŒ‡æ ‡å·²æŒ‰ç…§ä¼˜å…ˆçº§è¡¨å®Œæˆæµ‹é‡å’ŒæŠ¥å‘Š!")

if __name__ == "__main__":
    main()