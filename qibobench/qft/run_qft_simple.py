#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ç®€åŒ–ç‰ˆQiboåç«¯åŸºå‡†æµ‹è¯•è„šæœ¬
ä¸“æ³¨äºæ ¸å¿ƒæŒ‡æ ‡ï¼Œé¿å…å¤æ‚ä¾èµ–
"""

import time
import sys
import os
import platform
import json
import csv
from datetime import datetime
from qibo import Circuit, gates, set_backend

class SimpleBenchmark:
    """ç®€åŒ–ç‰ˆåŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.num_runs = 3  # è¿è¡Œæ¬¡æ•°
        self.warmup_runs = 1  # é¢„çƒ­æ¬¡æ•°
        self.baseline_backend = "numpy"
        self.results = {}
    
    def measure_memory_simple(self):
        """ç®€åŒ–çš„å†…å­˜æµ‹é‡æ–¹æ³•"""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024  # MB
        except ImportError:
            return 0.0  # å¦‚æœpsutilä¸å¯ç”¨ï¼Œè¿”å›0
    
    def load_qft_circuit(self):
        """åŠ è½½QFTç”µè·¯"""
        qasm_file = "QASMBench/medium/qft_n18/qft_n18_transpiled.qasm"
        
        if not os.path.exists(qasm_file):
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {qasm_file}")
            return None
        
        try:
            with open(qasm_file, "r") as file:
                qasm_code = file.read()
            
            # ç§»é™¤barrierè¯­å¥
            lines = qasm_code.split('\n')
            filtered_lines = [line for line in lines if 'barrier' not in line]
            clean_qasm_code = '\n'.join(filtered_lines)
            
            circuit = Circuit.from_qasm(clean_qasm_code)
            return circuit
            
        except Exception as e:
            print(f"åŠ è½½ç”µè·¯å¤±è´¥: {str(e)}")
            return None
    
    def run_backend_test(self, backend_name, platform_name=None):
        """è¿è¡Œå•ä¸ªåç«¯çš„æµ‹è¯•"""
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•åç«¯: {backend_name}")
        if platform_name:
            print(f"å¹³å°: {platform_name}")
        print('='*60)
        
        try:
            # è®¾ç½®åç«¯
            if platform_name is not None:
                set_backend(backend_name, platform=platform_name)
            else:
                set_backend(backend_name)
            
            # åŠ è½½ç”µè·¯
            circuit = self.load_qft_circuit()
            if circuit is None:
                return None
            
            # é¢„çƒ­è¿è¡Œ
            print("é¢„çƒ­è¿è¡Œ...")
            for _ in range(self.warmup_runs):
                _ = circuit()
            
            # æ­£å¼æµ‹è¯•
            print(f"æ­£å¼æµ‹è¯• ({self.num_runs}æ¬¡è¿è¡Œ)...")
            execution_times = []
            peak_memory = 0
            
            for run in range(self.num_runs):
                # æµ‹é‡å†…å­˜
                memory_before = self.measure_memory_simple()
                
                # æ‰§è¡Œç”µè·¯
                start_time = time.time()
                result = circuit()
                end_time = time.time()
                
                # æµ‹é‡å†…å­˜
                memory_after = self.measure_memory_simple()
                peak_memory = max(peak_memory, memory_after - memory_before)
                
                execution_time = end_time - start_time
                execution_times.append(execution_time)
                
                print(f"è¿è¡Œ {run+1}/{self.num_runs}: {execution_time:.4f}ç§’")
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            import numpy as np
            mean_time = np.mean(execution_times)
            std_time = np.std(execution_times)
            
            # è®¡ç®—ååç‡
            throughput = circuit.ngates / mean_time if mean_time > 0 else 0
            
            # éªŒè¯æ­£ç¡®æ€§
            correctness = "Passed" if result is not None else "Failed"
            
            return {
                "execution_time_mean": mean_time,
                "execution_time_std": std_time,
                "peak_memory_mb": peak_memory,
                "throughput_gates_per_sec": throughput,
                "correctness": correctness,
                "circuit_parameters": {
                    "nqubits": circuit.nqubits,
                    "depth": circuit.depth,
                    "ngates": circuit.ngates
                },
                "environment_info": {
                    "python_version": platform.python_version(),
                    "system": platform.system(),
                    "backend": backend_name,
                    "platform": platform_name or "default"
                }
            }
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
            return None
    
    def calculate_speedup(self):
        """è®¡ç®—åŠ é€Ÿæ¯”"""
        baseline_time = None
        
        # æŸ¥æ‰¾åŸºå‡†åç«¯çš„æ‰§è¡Œæ—¶é—´
        for backend_name, metrics in self.results.items():
            if backend_name == self.baseline_backend and metrics:
                baseline_time = metrics["execution_time_mean"]
                break
        
        if baseline_time:
            for backend_name, metrics in self.results.items():
                if metrics and backend_name != self.baseline_backend:
                    metrics["speedup"] = baseline_time / metrics["execution_time_mean"]
    
    def generate_csv_report(self, filename="simple_benchmark_report.csv"):
        """ç”ŸæˆCSVæŠ¥å‘Š"""
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # è¡¨å¤´
            headers = [
                "åç«¯åç§°", "æ‰§è¡Œæ—¶é—´å‡å€¼(ç§’)", "æ‰§è¡Œæ—¶é—´æ ‡å‡†å·®(ç§’)", 
                "å³°å€¼å†…å­˜å ç”¨(MB)", "åŠ é€Ÿæ¯”", "æ­£ç¡®æ€§éªŒè¯",
                "é‡å­æ¯”ç‰¹æ•°", "ç”µè·¯æ·±åº¦", "é—¨æ•°é‡", "ååç‡(é—¨/ç§’)"
            ]
            writer.writerow(headers)
            
            # æ•°æ®è¡Œ
            for backend_name, metrics in self.results.items():
                if metrics:
                    row = [
                        backend_name,
                        f"{metrics['execution_time_mean']:.6f}",
                        f"{metrics['execution_time_std']:.6f}",
                        f"{metrics['peak_memory_mb']:.2f}",
                        f"{metrics.get('speedup', 'N/A'):.2f}x" if metrics.get('speedup') else "N/A",
                        metrics['correctness'],
                        metrics['circuit_parameters']['nqubits'],
                        metrics['circuit_parameters']['depth'],
                        metrics['circuit_parameters']['ngates'],
                        f"{metrics['throughput_gates_per_sec']:.2f}"
                    ]
                    writer.writerow(row)
        
        print(f"CSVæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
    
    def generate_markdown_report(self, filename="simple_benchmark_report.md"):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as md_file:
            # æ ‡é¢˜
            md_file.write("# Qiboåç«¯åŸºå‡†æµ‹è¯•æŠ¥å‘Š (ç®€åŒ–ç‰ˆ)\n\n")
            md_file.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æ ¸å¿ƒæŒ‡æ ‡è¡¨æ ¼
            md_file.write("## æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡\n\n")
            md_file.write("| åç«¯ | æ‰§è¡Œæ—¶é—´(ç§’) | å†…å­˜(MB) | åŠ é€Ÿæ¯” | æ­£ç¡®æ€§ | ååç‡(é—¨/ç§’) |\n")
            md_file.write("|------|-------------|----------|--------|--------|---------------|\n")
            
            for backend_name, metrics in self.results.items():
                if metrics:
                    time_str = f"{metrics['execution_time_mean']:.4f} Â± {metrics['execution_time_std']:.4f}"
                    memory_str = f"{metrics['peak_memory_mb']:.1f}"
                    speedup_str = f"{metrics.get('speedup', 'N/A'):.2f}x" if metrics.get('speedup') else "N/A"
                    throughput_str = f"{metrics['throughput_gates_per_sec']:.0f}"
                    
                    md_file.write(f"| {backend_name} | {time_str} | {memory_str} | {speedup_str} | {metrics['correctness']} | {throughput_str} |\n")
            
            # ç¯å¢ƒä¿¡æ¯
            md_file.write("\n## æµ‹è¯•ç¯å¢ƒ\n\n")
            for backend_name, metrics in self.results.items():
                if metrics:
                    md_file.write(f"### {backend_name}\n")
                    for key, value in metrics['environment_info'].items():
                        md_file.write(f"- {key}: {value}\n")
                    md_file.write("\n")
        
        print(f"MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ç®€åŒ–ç‰ˆQiboåç«¯åŸºå‡†æµ‹è¯•")
        print(f"æµ‹è¯•é…ç½®: {self.num_runs}æ¬¡è¿è¡Œ, {self.warmup_runs}æ¬¡é¢„çƒ­")
        print(f"åŸºå‡†åç«¯: {self.baseline_backend}")
        print('='*60)
        
        # åç«¯é…ç½®
        backend_configs = {
            "numpy": {"backend_name": "numpy", "platform_name": None},
            "qibojit (numba)": {"backend_name": "qibojit", "platform_name": "numba"},
            "qibotn (qutensornet)": {"backend_name": "qibotn", "platform_name": "qutensornet"},
            "qiboml (jax)": {"backend_name": "qiboml", "platform_name": "jax"},
            "qiboml (pytorch)": {"backend_name": "qiboml", "platform_name": "pytorch"},
            "qiboml (tensorflow)": {"backend_name": "qiboml", "platform_name": "tensorflow"}
        }
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        for backend_key, config in backend_configs.items():
            result = self.run_backend_test(
                config["backend_name"], 
                config["platform_name"]
            )
            self.results[backend_key] = result
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        self.calculate_speedup()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_csv_report()
        self.generate_markdown_report()
        
        # æ‰“å°æ€»ç»“
        self.print_summary()
        
        return self.results
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "="*60)
        print("ğŸ“Š åŸºå‡†æµ‹è¯•æ€»ç»“")
        print("="*60)
        
        successful_tests = {k: v for k, v in self.results.items() if v is not None}
        
        if successful_tests:
            print("æˆåŠŸæµ‹è¯•çš„åç«¯ (æŒ‰æ‰§è¡Œæ—¶é—´æ’åº):")
            sorted_results = sorted(successful_tests.items(), 
                                   key=lambda x: x[1]["execution_time_mean"])
            
            for i, (backend_name, metrics) in enumerate(sorted_results, 1):
                speedup_str = f" ({metrics.get('speedup', 'N/A'):.2f}x)" if metrics.get('speedup') else ""
                print(f"{i}. {backend_name}: {metrics['execution_time_mean']:.4f}ç§’{speedup_str}")
        
        print(f"\næŠ¥å‘Šæ–‡ä»¶å·²ç”Ÿæˆ:")
        print("  - simple_benchmark_report.csv")
        print("  - simple_benchmark_report.md")
        print("\nğŸ¯ åŸºå‡†æµ‹è¯•å®Œæˆ!")

def main():
    """ä¸»å‡½æ•°"""
    benchmark = SimpleBenchmark()
    results = benchmark.run_all_tests()

if __name__ == "__main__":
    main()